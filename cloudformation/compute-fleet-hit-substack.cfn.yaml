{%- set queues = config.cluster.queue_settings -%}
{%- set scaling_config = config.cluster.scaling -%}
AWSTemplateFormatVersion: '2010-09-09'
Parameters:
  ComputeSubnetId:
    Type: String
  RootDevice:
    Type: String
  RootVolumeSize:
    Type: Number
  KeyName:
    Type: String
  MainStackName:
    Type: String
  AttributesTagValue:
    Type: String
  NetworkingTagValue:
    Type: String
  FilesystemTagValue:
    Type: String
  ImageId:
    Type: AWS::EC2::Image::Id
  IamInstanceProfile:
    Type: String
  BaseOS:
    Type: String
  OSUser:
    Type: String
  PreInstallScript:
    Type: String
  PreInstallArgs:
    Type: String
  PostInstallScript:
    Type: String
  PostInstallArgs:
    Type: String
  RAIDOptions:
    Type: String
  EFSId:
    Type: String
  EFSOptions:
    Type: String
  FSXId:
    Type: String
  FSXOptions:
    Type: String
  Scheduler:
    Type: String
  EncryptedEphemeral:
    Type: String
  EphemeralDir:
    Type: String
  SharedDir:
    Type: String
  ProxyServer:
    Type: String
  ClusterDNSDomain:
    Type: String
  IntelHPCPlatform:
    Type: String
  CWLoggingEnabled:
    Type: String
  ExtraJson:
    Type: String
  CustomChefCookbook:
    Type: String
  AWSDomain:
    Type: String
  ParallelClusterVersion:
    Type: String
  CookbookVersion:
    Type: String
  ChefVersion:
    Type: String
  BerkshelfVersion:
    Type: String
  ComputeSecurityGroups:
    Type: List<AWS::EC2::SecurityGroup::Id>
  AssociatePublicIpAddress:
    Type: String
  VPCId:
    Type: AWS::EC2::VPC::Id
  RootRole:
    Type: String
  ResourcesS3Bucket:
    Type: String
Conditions:
  UseProxy: !Not
    - !Equals
      - !Ref 'ProxyServer'
      - NONE
  AddRoute53IamPolicies: !Not
    - !Equals
      - !Ref 'RootRole'
      - NONE
Resources:
{%- for queue, queue_config in queues.items() %}
  {%- for compute_resource in queue_config.compute_resource_settings.values() %}
    {%- set instance_type = compute_resource.instance_type %}
  ComputeServerLaunchTemplate{{ (queue + instance_type) | sha1 | truncate(16, True, "") | capitalize }}:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateName: !Sub
        - ${ClusterName}-{{ queue }}-{{ instance_type }}
        - ClusterName: !Select
            - '1'
            - !Split
              - parallelcluster-
              - !Ref 'MainStackName'
      LaunchTemplateData:
        InstanceType: {{ instance_type }}
        NetworkInterfaces:
          - DeviceIndex: 0
            InterfaceType: {{ 'efa' if compute_resource.enable_efa else "!Ref 'AWS::NoValue'" }}
            Groups: !Ref 'ComputeSecurityGroups'
            AssociatePublicIpAddress: !Ref 'AssociatePublicIpAddress'
            SubnetId: !Ref 'ComputeSubnetId'
        Placement:
          GroupName: {% if queue_config.placement_group == "DYNAMIC" %}!Ref 'PlacementGroup{{ queue | sha1 | truncate(16, True, "") | capitalize }}'{% elif queue_config.placement_group %}{{ queue_config.placement_group }}{% else %}!Ref 'AWS::NoValue'{% endif %}
        KeyName: !Ref 'KeyName'
        IamInstanceProfile:
          Name: !Ref 'IamInstanceProfile'
  {%- if queue_config.compute_type == "spot" %}
        InstanceMarketOptions:
          SpotOptions:
            SpotInstanceType: one-time
            InstanceInterruptionBehavior: terminate
            MaxPrice: {% if compute_resource.spot_price %}'{{ compute_resource.spot_price }}'{% else %}!Ref 'AWS::NoValue'{% endif %}
          MarketType: spot
  {%- endif %}
        ImageId: !Ref 'ImageId'
        CpuOptions:
          CoreCount: {{ compute_resource.vcpus if compute_resource.disable_hyperthreading_via_cpu_options else "!Ref 'AWS::NoValue'" }}
          ThreadsPerCore: {{ 1 if compute_resource.disable_hyperthreading_via_cpu_options else "!Ref 'AWS::NoValue'" }}
        Monitoring:
          Enabled: false
        TagSpecifications:
          - ResourceType: instance
            Tags:
              - Key: Name
                Value: Compute
              - Key: ClusterName
                Value: !Select
                  - '1'
                  - !Split
                    - parallelcluster-
                    - !Ref 'MainStackName'
              - Key: QueueName
                Value: {{ queue }}
              - Key: Application
                Value: !Ref 'MainStackName'
              - Key: aws-parallelcluster-attributes
                Value: !Ref 'AttributesTagValue'
              - Key: aws-parallelcluster-networking
                Value: !Ref 'NetworkingTagValue'
              - Key: aws-parallelcluster-filesystem
                Value: !Ref 'FilesystemTagValue'
              - Key: aws-parallelcluster-node-type
                Value: Compute
        BlockDeviceMappings:
          - DeviceName: /dev/xvdba
            VirtualName: ephemeral0
          - DeviceName: /dev/xvdbb
            VirtualName: ephemeral1
          - DeviceName: /dev/xvdbc
            VirtualName: ephemeral2
          - DeviceName: /dev/xvdbd
            VirtualName: ephemeral3
          - DeviceName: /dev/xvdbe
            VirtualName: ephemeral4
          - DeviceName: /dev/xvdbf
            VirtualName: ephemeral5
          - DeviceName: /dev/xvdbg
            VirtualName: ephemeral6
          - DeviceName: /dev/xvdbh
            VirtualName: ephemeral7
          - DeviceName: /dev/xvdbi
            VirtualName: ephemeral8
          - DeviceName: /dev/xvdbj
            VirtualName: ephemeral9
          - DeviceName: /dev/xvdbk
            VirtualName: ephemeral10
          - DeviceName: /dev/xvdbl
            VirtualName: ephemeral11
          - DeviceName: /dev/xvdbm
            VirtualName: ephemeral12
          - DeviceName: /dev/xvdbn
            VirtualName: ephemeral13
          - DeviceName: /dev/xvdbo
            VirtualName: ephemeral14
          - DeviceName: /dev/xvdbp
            VirtualName: ephemeral15
          - DeviceName: /dev/xvdbq
            VirtualName: ephemeral16
          - DeviceName: /dev/xvdbr
            VirtualName: ephemeral17
          - DeviceName: /dev/xvdbs
            VirtualName: ephemeral18
          - DeviceName: /dev/xvdbt
            VirtualName: ephemeral19
          - DeviceName: /dev/xvdbu
            VirtualName: ephemeral20
          - DeviceName: /dev/xvdbv
            VirtualName: ephemeral21
          - DeviceName: /dev/xvdbw
            VirtualName: ephemeral22
          - DeviceName: /dev/xvdbx
            VirtualName: ephemeral23
          - DeviceName: !Ref 'RootDevice'
            Ebs:
              VolumeSize: !Ref 'RootVolumeSize'
              VolumeType: gp2
        UserData: !Base64
          Fn::Sub:
            - |
              Content-Type: multipart/mixed; boundary="==BOUNDARY=="
              MIME-Version: 1.0

              --==BOUNDARY==
              Content-Type: text/cloud-boothook; charset="us-ascii"
              MIME-Version: 1.0

              #!/bin/bash -x

              which yum && echo "proxy=${YumProxy}" >> /etc/yum.conf || echo "Not yum system"

              which apt-get && echo "Acquire::http::Proxy \"${AptProxy}\";" >> /etc/apt/apt.conf || echo "Not apt system"

              proxy=${ProxyServer}
              if [ "${!proxy}" != "NONE" ]; then
                proxy_host=$(echo "${!proxy}" | awk -F/ '{print $3}' | cut -d: -f1)
                proxy_port=$(echo "${!proxy}" | awk -F/ '{print $3}' | cut -d: -f2)
                echo -e "[Boto]\nproxy = ${!proxy_host}\nproxy_port = ${!proxy_port}\n" >/etc/boto.cfg
                cat >> /etc/profile.d/proxy.sh <<PROXY
              export http_proxy="${!proxy}"
              export https_proxy="${!proxy}"
              export no_proxy="localhost,127.0.0.1,169.254.169.254"
              export HTTP_PROXY="${!proxy}"
              export HTTPS_PROXY="${!proxy}"
              export NO_PROXY="localhost,127.0.0.1,169.254.169.254"
              PROXY
              fi

              --==BOUNDARY==
              Content-Type: text/cloud-config; charset=us-ascii
              MIME-Version: 1.0

              output:
                all: "| tee -a /var/log/cloud-init-output.log | logger -t user-data -s 2>/dev/console"
              write_files:
                - path: /tmp/dna.json
                  permissions: '0644'
                  owner: root:root
                  content: |
                    {
                      "cfncluster": {
                        "stack_name": "${MainStackName}",
                        "enable_efa": "{{ 'efa' if compute_resource.enable_efa else 'NONE' }}",
                        "cfn_raid_parameters": "${RAIDOptions}",
                        "cfn_base_os": "${BaseOS}",
                        "cfn_preinstall": "${PreInstallScript}",
                        "cfn_preinstall_args": "${PreInstallArgs}",
                        "cfn_postinstall": "${PostInstallScript}",
                        "cfn_postinstall_args": "${PostInstallArgs}",
                        "cfn_region": "${AWS::Region}",
                        "cfn_efs": "${EFSId}",
                        "cfn_efs_shared_dir": "${EFSOptions}",
                        "cfn_fsx_fs_id": "${FSXId}",
                        "cfn_fsx_options": "${FSXOptions}",
                        "cfn_scheduler": "${Scheduler}",
                        "cfn_scheduler_slots": "{{ compute_resource.vcpus }}",
                        "cfn_disable_hyperthreading_manually": "{{ (compute_resource.disable_hyperthreading and not compute_resource.disable_hyperthreading_via_cpu_options) | lower }}",
                        "cfn_scaledown_idletime": "{{ scaling_config.scaledown_idletime }}",
                        "cfn_encrypted_ephemeral": "${EncryptedEphemeral}",
                        "cfn_ephemeral_dir": "${EphemeralDir}",
                        "cfn_shared_dir": "${SharedDir}",
                        "cfn_proxy": "${ProxyServer}",
                        "cfn_ddb_table": "${DynamoDBTable}",
                        "cfn_dns_domain": {% if config.cluster.disable_cluster_dns == False %}"${ClusterDNSDomain}"{% else %}""{% endif %},
                        "cfn_hosted_zone": {% if config.cluster.disable_cluster_dns == False %}"${ClusterHostedZone}"{% else %}""{% endif %},
                        "cfn_node_type": "ComputeFleet",
                        "cfn_cluster_user": "${OSUser}",
                        "enable_intel_hpc_platform": "${IntelHPCPlatform}",
                        "cfn_cluster_cw_logging_enabled": "${CWLoggingEnabled}",
                        "scheduler_queue_name": "{{ queue }}"
                      },
                      "run_list": "recipe[aws-parallelcluster::${Scheduler}_config]"
                    }

                - path: /etc/chef/client.rb
                  permissions: '0644'
                  owner: root:root
                  content: cookbook_path ['/etc/chef/cookbooks']
                - path: /tmp/extra.json
                  permissions: '0644'
                  owner: root:root
                  content: |
                    ${ExtraJson}

              --==BOUNDARY==
              Content-Type: text/x-shellscript; charset="us-ascii"
              MIME-Version: 1.0

              #!/bin/bash -x

              function error_exit
              {
                region=${AWS::Region}
                instance_id=$(curl --retry 3 --retry-delay 0 --silent --fail http://169.254.169.254/latest/meta-data/instance-id)
                log_dir=/home/logs/compute
                mkdir -p ${!log_dir}
                echo "Reporting instance as unhealthy and dumping logs to ${!log_dir}/${!instance_id}.tar.gz"
                tar -czf ${!log_dir}/${!instance_id}.tar.gz /var/log
                # TODO: add possibility to disable this behavior
                # wait logs flush before signaling the failure
                sleep 10
                aws --region ${!region} ec2 terminate-instances --instance-ids ${!instance_id}
                exit 1
              }
              function vendor_cookbook
              {
                mkdir /tmp/cookbooks
                cd /tmp/cookbooks
                tar -xzf /etc/chef/aws-parallelcluster-cookbook.tgz
                HOME_BAK="${!HOME}"
                export HOME="/tmp"
                for d in `ls /tmp/cookbooks`; do
                  cd /tmp/cookbooks/$d
                  LANG=en_US.UTF-8 /opt/cinc/embedded/bin/berks vendor /etc/chef/cookbooks --delete || error_exit 'Vendoring cookbook failed.'
                done;
                export HOME="${!HOME_BAK}"
              }
              function bootstrap_instance
              {
                which dnf 2>/dev/null; dnf=$?
                which yum 2>/dev/null; yum=$?
                which apt-get 2>/dev/null; apt=$?
                if [ "${!dnf}" == "0" ]; then
                  dnf -y groupinstall development && dnf -y install curl wget jq python3-pip
                  pip3 install awscli --upgrade --user
                elif [ "${!yum}" == "0" ]; then
                  yum -y groupinstall development && yum -y install curl wget jq awscli python3-pip
                fi
                if [ "${!apt}" == "0" ]; then
                  apt-cache search build-essential; apt-get clean; apt-get update; apt-get -y install build-essential curl wget jq python-setuptools awscli python3-pip
                fi
                [[ ${!_region} =~ ^cn- ]] && s3_url="cn-north-1.amazonaws.com.cn/cn-north-1-aws-parallelcluster"
                which cfn-init 2>/dev/null || ( curl -s -L -o /tmp/aws-cfn-bootstrap-py3-latest.tar.gz https://s3.${!s3_url}/cloudformation-examples/aws-cfn-bootstrap-py3-latest.tar.gz; pip3 install -U /tmp/aws-cfn-bootstrap-py3-latest.tar.gz)
                mkdir -p /etc/chef && chown -R root:root /etc/chef
                curl --retry 3 -L https://${!_region}-aws-parallelcluster.s3.${!_region}.amazonaws.com$([ "${!_region}" != "${!_region#cn-*}" ] && echo ".cn" || exit 0)/archives/cinc/cinc-install.sh | bash -s -- -v ${!chef_version}
                /opt/cinc/embedded/bin/gem install --no-document berkshelf:${!berkshelf_version}
                curl --retry 3 -s -L -o /etc/chef/aws-parallelcluster-cookbook.tgz ${!cookbook_url}
                curl --retry 3 -s -L -o /etc/chef/aws-parallelcluster-cookbook.tgz.date ${!cookbook_url}.date
                curl --retry 3 -s -L -o /etc/chef/aws-parallelcluster-cookbook.tgz.md5 ${!cookbook_url}.md5
                vendor_cookbook
                mkdir /opt/parallelcluster
              }
              [ -f /etc/profile.d/proxy.sh ] && . /etc/profile.d/proxy.sh
              custom_cookbook=${CustomChefCookbook}
              export _region=${AWS::Region}
              s3_url=${AWSDomain}
              if [ "${!custom_cookbook}" != "NONE" ]; then
                cookbook_url=${!custom_cookbook}
              else
                cookbook_url=https://s3.${!_region}.${!s3_url}/${!_region}-aws-parallelcluster/cookbooks/${CookbookVersion}.tgz
              fi
              export PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/opt/aws/bin
              export parallelcluster_version=aws-parallelcluster-${ParallelClusterVersion}
              export cookbook_version=${CookbookVersion}
              export chef_version=${ChefVersion}
              export berkshelf_version=${BerkshelfVersion}
              if [ -f /opt/parallelcluster/.bootstrapped ]; then
                installed_version=$(cat /opt/parallelcluster/.bootstrapped)
                if [ "${!parallelcluster_version}" != "${!installed_version}" ]; then
                  error_exit "This AMI was created with ${!installed_version}, but is trying to be used with ${!parallelcluster_version}. Please either use an AMI created with ${!parallelcluster_version} or change your ParallelCluster to ${!installed_version}"
                fi
              else
                bootstrap_instance
              fi
              if [ "${!custom_cookbook}" != "NONE" ]; then
                curl --retry 3 -v -L -o /etc/chef/aws-parallelcluster-cookbook.tgz -z "$(cat /etc/chef/aws-parallelcluster-cookbook.tgz.date)" ${!cookbook_url}
                vendor_cookbook
              fi
              cd /tmp

              mkdir -p /etc/chef/ohai/hints
              touch /etc/chef/ohai/hints/ec2.json
              jq --argfile f1 /tmp/dna.json --argfile f2 /tmp/extra.json -n '$f1 + $f2 | .cfncluster = $f1.cfncluster + $f2.cfncluster' > /etc/chef/dna.json || ( echo "jq not installed or invalid extra_json"; cp /tmp/dna.json /etc/chef/dna.json)
              {
                pushd /etc/chef &&
                chef-client --local-mode --config /etc/chef/client.rb --log_level auto --force-formatter --no-color --chef-zero-port 8889 --json-attributes /etc/chef/dna.json --override-runlist aws-parallelcluster::prep_env &&
                /opt/parallelcluster/scripts/fetch_and_run -preinstall &&
                chef-client --local-mode --config /etc/chef/client.rb --log_level auto --force-formatter --no-color --chef-zero-port 8889 --json-attributes /etc/chef/dna.json &&
                /opt/parallelcluster/scripts/fetch_and_run -postinstall &&
                chef-client --local-mode --config /etc/chef/client.rb --log_level auto --force-formatter --no-color --chef-zero-port 8889 --json-attributes /etc/chef/dna.json --override-runlist aws-parallelcluster::finalize &&
                popd
              } || error_exit 'Failed to run bootstrap recipes. If --norollback was specified, check /var/log/cfn-init.log and /var/log/cloud-init-output.log.'

              if [ ! -f /opt/parallelcluster/.bootstrapped ]; then
                echo ${!parallelcluster_version} | tee /opt/parallelcluster/.bootstrapped
              fi
              # End of file
              --==BOUNDARY==
            - YumProxy: !If
                - UseProxy
                - !Ref 'ProxyServer'
                - ''
              AptProxy: !If
                - UseProxy
                - !Ref 'ProxyServer'
                - 'false'
  {%- endfor %}
{%- endfor %}
{%- for queue, queue_config in queues.items() %}
  {%- if queue_config.placement_group == "DYNAMIC" %}
  PlacementGroup{{ queue | sha1 | truncate(16, True, "") | capitalize }}:
    Type: AWS::EC2::PlacementGroup
    Properties:
      Strategy: cluster
  {%- endif %}
{%- endfor %}
  DynamoDBTable:
    Type: AWS::DynamoDB::Table
    UpdateReplacePolicy: Retain
    DeletionPolicy: Delete
    Properties:
      TableName: !Ref 'MainStackName'
      AttributeDefinitions:
        - AttributeName: Id
          AttributeType: S
        - AttributeName: InstanceId
          AttributeType: S
      KeySchema:
        - AttributeName: Id
          KeyType: HASH
      GlobalSecondaryIndexes:
        - IndexName: InstanceId
          KeySchema:
            - AttributeName: InstanceId
              KeyType: HASH
          Projection:
            ProjectionType: ALL
      BillingMode: PAY_PER_REQUEST
  {%- if config.cluster.disable_cluster_dns == False %}
  ClusterHostedZone:
    Type: AWS::Route53::HostedZone
    Properties:
      Name: !Ref 'ClusterDNSDomain'
      VPCs:
        - VPCId: !Ref 'VPCId'
          VPCRegion: !Ref 'AWS::Region'
  ParallelClusterHITRoute53Policies:
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: parallelcluster-hit-route53
      PolicyDocument:
        Statement:
          - Sid: Route53Add
            Action:
              - route53:ChangeResourceRecordSets
            Effect: Allow
            Resource:
              - !Sub
                - arn:${AWS::Partition}:route53:::hostedzone/${ClusterHostedZone}
                - ClusterHostedZone: !Ref 'ClusterHostedZone'
      Roles:
        - !Ref 'RootRole'
    Condition: AddRoute53IamPolicies
  CleanupRoute53Function:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub
        - pcluster-CleanupRoute53-${StackId}
        - StackId: !Select
            - '2'
            - !Split
              - /
              - !Ref 'AWS::StackId'
      Code:
        S3Bucket: !Ref 'ResourcesS3Bucket'
        S3Key: custom_resources_code/artifacts.zip
      Handler: cleanup_resources.handler
      MemorySize: 128
      Role: !GetAtt 'CleanupRoute53FunctionExecutionRole.Arn'
      Runtime: python3.8
      Timeout: 900
  CleanupRoute53CustomResource:
    Type: AWS::CloudFormation::CustomResource
    Properties:
      ClusterHostedZone: !Ref 'ClusterHostedZone'
      Action: DELETE_DNS_RECORDS
      ServiceToken: !GetAtt 'CleanupRoute53Function.Arn'
  CleanupRoute53FunctionExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: '2012-10-17'
      Path: /
      Policies:
        - PolicyDocument:
            Statement:
              - Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Effect: Allow
                Resource: !Sub 'arn:${AWS::Partition}:logs:*:*:*'
                Sid: CloudWatchLogsPolicy
              - Sid: Route53DeletePolicy
                Effect: Allow
                Action:
                  - route53:ListResourceRecordSets
                  - route53:ChangeResourceRecordSets
                Resource:
                  - !Sub
                    - arn:${AWS::Partition}:route53:::hostedzone/${ClusterHostedZone}
                    - ClusterHostedZone: !Ref 'ClusterHostedZone'
            Version: '2012-10-17'
          PolicyName: LambdaPolicy
  {%- endif %}
  UpdateWaiterFunction:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        S3Bucket: !Ref 'ResourcesS3Bucket'
        S3Key: custom_resources_code/artifacts.zip
      Handler: wait_for_update.handler
      MemorySize: 128
      Role: !GetAtt 'UpdateWaiterFunctionExecutionRole.Arn'
      Runtime: python3.8
      Timeout: 900
  UpdateWaiterFunctionExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: '2012-10-17'
      Path: /
      Policies:
        - PolicyDocument:
            Statement:
              - Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Effect: Allow
                Resource: !Sub 'arn:${AWS::Partition}:logs:*:*:*'
                Sid: CloudWatchLogsPolicy
              - Sid: DynamoDBTable
                Effect: Allow
                Action:
                  - dynamodb:GetItem
                  - dynamodb:PutItem
                Resource: !Sub 'arn:${AWS::Partition}:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${DynamoDBTable}'
            Version: '2012-10-17'
          PolicyName: LambdaPolicy
Metadata:
  RootRole: !Ref 'RootRole'
  VPCId: !Ref 'VPCId'
  ClusterDNSDomain: !Ref 'ClusterDNSDomain'
  ResourcesS3Bucket: !Ref 'ResourcesS3Bucket'
  AddRoute53IamPolicies: !If
    - AddRoute53IamPolicies
    - true
    - false
Outputs:
  ClusterHostedZone:
    Description: Id of the private hosted zone created within the cluster
    Value: {% if config.cluster.disable_cluster_dns == False %}!Ref 'ClusterHostedZone'{% else %}''{% endif %}
  ClusterDNSDomain:
    Description: DNS Domain of the private hosted zone created within the cluster
    Value: {% if config.cluster.disable_cluster_dns == False %}!Ref 'ClusterDNSDomain'{% else %}''{% endif %}
  CleanupRoute53CustomResource:
    Description: ARN of the custom resource to cleanup Route53
    Value: {% if config.cluster.disable_cluster_dns == False %}!Ref 'CleanupRoute53CustomResource'{% else %}''{% endif %}
  ConfigVersion:
    Description: Version of the config used to generate the HIT substack
    Value: {{ config_version }}
  UpdateWaiterFunctionArn:
    Value: !GetAtt 'UpdateWaiterFunction.Arn'

